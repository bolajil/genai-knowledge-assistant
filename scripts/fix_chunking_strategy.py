"""
Fix Chunking Strategy - Use larger chunks to preserve complete sections
"""

import logging
import sys
import json
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.semantic_chunking_strategy import create_semantic_chunks

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def fix_bylaw_chunking():
    """Re-chunk ByLaw document with larger, section-preserving chunks"""
    
    print("üîß Fixing ByLaw Chunking Strategy")
    print("=" * 50)
    
    try:
        # Load existing ByLaw document
        bylaw_index_path = Path("data/indexes/ByLaw01_index")
        extracted_text_path = bylaw_index_path / "extracted_text.txt"
        
        if not extracted_text_path.exists():
            print("‚ùå ByLaw extracted text not found")
            return False
        
        print("üìÑ Loading ByLaw document...")
        with open(extracted_text_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"üìä Document length: {len(content)} characters")
        
        # Remove existing semantic chunks
        print("üßπ Removing existing fragmented chunks...")
        chunk_files = list(bylaw_index_path.glob("semantic_chunk_*.json"))
        print(f"   Found {len(chunk_files)} existing chunks to remove")
        
        for chunk_file in chunk_files:
            chunk_file.unlink()
        
        # Apply optimized semantic chunking
        print("üß† Applying optimized semantic chunking...")
        print("   ‚Ä¢ Chunk size: 1500 characters (larger to preserve sections)")
        print("   ‚Ä¢ Chunk overlap: 300 characters (more overlap for context)")
        print("   ‚Ä¢ Split by headers: Enabled")
        print("   ‚Ä¢ Embeddings: Enabled")
        
        chunks = create_semantic_chunks(
            content=content,
            document_name="ByLaw01_index",
            chunk_size=1500,  # Larger chunks to preserve complete sections
            chunk_overlap=300  # More overlap for better context
        )
        
        if not chunks:
            print("‚ùå No chunks created")
            return False
        
        print(f"‚úÖ Created {len(chunks)} optimized semantic chunks")
        
        # Save optimized chunks
        print("üíæ Saving optimized chunks...")
        for i, chunk in enumerate(chunks):
            chunk_file = bylaw_index_path / f"semantic_chunk_{i+1}.json"
            with open(chunk_file, "w", encoding="utf-8") as f:
                json.dump(chunk, f, indent=2, ensure_ascii=False)
        
        # Update metadata
        metadata = {
            'total_chunks': len(chunks),
            'chunking_method': 'semantic_optimized',
            'chunk_size': 1500,
            'chunk_overlap': 300,
            'split_by_headers': True,
            'use_embeddings': True,
            'document_name': 'ByLaw01_index',
            'optimization': 'section_preserving',
            'chunks': []
        }
        
        # Analyze chunks for section preservation
        section_chunks = 0
        powers_found = False
        
        for i, chunk in enumerate(chunks):
            chunk_info = {
                'chunk_id': i + 1,
                'title': chunk['title'],
                'size': chunk['chunk_size'],
                'has_embedding': chunk.get('has_embedding', False),
                'chunk_type': chunk['chunk_type'],
                'hierarchy_level': len(chunk.get('hierarchy', [])),
                'contains_powers': 'powers' in chunk['content'].lower()
            }
            metadata['chunks'].append(chunk_info)
            
            if chunk['chunk_type'] == 'semantic_header':
                section_chunks += 1
            
            if 'powers' in chunk['content'].lower():
                powers_found = True
                print(f"   ‚úÖ Found 'Powers' section in chunk {i+1}: {chunk['title']}")
        
        # Save updated metadata
        with open(bylaw_index_path / "chunks_metadata.json", "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2)
        
        print("\nüìã Optimization Results:")
        print(f"   ‚Ä¢ Total chunks: {len(chunks)} (reduced from 238)")
        print(f"   ‚Ä¢ Section-based chunks: {section_chunks}")
        print(f"   ‚Ä¢ Powers section preserved: {'‚úÖ Yes' if powers_found else '‚ùå No'}")
        print(f"   ‚Ä¢ Average chunk size: {sum(c['chunk_size'] for c in chunks) // len(chunks)} chars")
        
        # Show chunks containing "powers"
        powers_chunks = [c for c in chunks if 'powers' in c['content'].lower()]
        if powers_chunks:
            print(f"\nüéØ Powers-related chunks ({len(powers_chunks)}):")
            for i, chunk in enumerate(powers_chunks):
                print(f"   Chunk {chunks.index(chunk)+1}: {chunk['title']}")
                print(f"   Size: {chunk['chunk_size']} chars")
                print(f"   Preview: {chunk['content'][:150]}...")
                print()
        
        print(f"‚úÖ ByLaw chunking strategy optimized!")
        print(f"üìÅ Optimized chunks saved to: {bylaw_index_path}")
        
        return True
        
    except Exception as e:
        logger.error(f"Error fixing chunking strategy: {e}")
        print(f"‚ùå Error: {e}")
        return False

if __name__ == "__main__":
    success = fix_bylaw_chunking()
    sys.exit(0 if success else 1)
